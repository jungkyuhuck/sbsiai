import openai
import numpy as np
import faiss
import pickle
import kss
from pymongo import MongoClient
import time
import os
from dotenv import load_dotenv  # âœ… ì¶”ê°€

# âœ… .env íŒŒì¼ì—ì„œ OpenAI API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# âœ… ì„ë² ë”© í•¨ìˆ˜
def get_embedding(text: str) -> list:
    try:
        res = openai.Embedding.create(input=text, model="text-embedding-ada-002")
        return res["data"][0]["embedding"]
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨: {text[:30]}... â†’ {e}")
        return None

# âœ… MongoDB â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def fetch_episodes_and_characters(uri: str, db: str, coll: str) -> list:
    client = MongoClient(uri)
    col = client[db][coll]
    texts = []

    for doc in col.find({}):
        title = doc.get("title") or doc.get("program_title") or "UNKNOWN"
        season = doc.get("season", 0)

        # episodes
        for ep in doc.get("episodes", []):
            ep_no = ep.get("episode", "UNKNOWN")
            # summary
            summary = ep.get("summary", "")
            for s in kss.split_sentences(summary):
                texts.append(f"[{title} S{season}E{ep_no}] ì¤„ê±°ë¦¬: {s.strip()}")
            # vtt
            vtt = ep.get("vtt", "")
            for s in kss.split_sentences(vtt):
                texts.append(f"[{title} S{season}E{ep_no}] ëŒ€ì‚¬: {s.strip()}")

        # characters
        for ch in doc.get("characters", []):
            name = ch.get("name", "UNKNOWN")
            for s in kss.split_sentences(ch.get("description", "")):
                texts.append(f"[{title} ìºë¦­í„° {name}] ì„¤ëª…: {s.strip()}")

    return texts

# âœ… ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥
def build_and_save_index(texts: list,
                         faiss_path: str = "sbs_index.faiss",
                         meta_path: str = "sbs_metadata.pkl",
                         vec_path: str = "sbs_vectors.npy"):
    vectors, metadata = [], []
    for t in texts:
        emb = get_embedding(t)
        if emb:
            vectors.append(emb)
            metadata.append(t)
        time.sleep(0.05)  # API ë¶€í•˜ ì™„í™”

    if not vectors:
        print("âŒ ì„ë² ë”©ëœ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    vecs = np.array(vectors).astype("float32")
    vecs /= np.linalg.norm(vecs, axis=1, keepdims=True)  # ğŸ”„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì •ê·œí™”

    index = faiss.IndexFlatIP(vecs.shape[1])  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš© ë‚´ì  ì¸ë±ìŠ¤
    index.add(vecs)

    # â–¶ ì €ì¥
    faiss.write_index(index, faiss_path)
    with open(meta_path, "wb") as f:
        pickle.dump(metadata, f)
    np.save(vec_path, vecs)

    print(f"âœ… ë²¡í„° {len(vecs)}ê°œ, ë©”íƒ€ë°ì´í„° {len(metadata)}ê°œ ì €ì¥ ì™„ë£Œ")

# ===== ì‹¤í–‰ =====
if __name__ == "__main__":
    URI = "mongodb://43.201.154.108:27017"
    DB = "aiproject"
    COLL = "deep learning_data"
    txts = fetch_episodes_and_characters(URI, DB, COLL)
    build_and_save_index(txts)
